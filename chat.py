"""
Simple Multi-Agent Chat System
Single-file prototype that implements:
- Coordinator
- ResearchAgent
- AnalysisAgent
- MemoryAgent (with simple in-memory vector store)
- A lightweight KnowledgeBase (mock documents)
- Test scenarios and outputs written to outputs/*.txt
- Functions to generate README, Dockerfile, docker-compose.yaml

Run: python3 multi_agent_chat_system.py

This script will:
- Execute the five sample test scenarios described in the assessment
- Print trace logs to console
- Save traces to outputs/*.txt
- Create supporting files (README.md, Dockerfile, docker-compose.yaml)

Author: Generated by ChatGPT
"""

import os
import json
import time
import math
import shutil
from datetime import datetime
from collections import Counter, defaultdict
from typing import List, Dict, Any, Tuple

# --------------------------- Utilities ---------------------------

def now_ts() -> str:
    return datetime.utcnow().isoformat() + "Z"


def cosine_sim(a: List[float], b: List[float]) -> float:
    # compute cosine similarity
    num = sum(x*y for x,y in zip(a,b))
    sum1 = math.sqrt(sum(x*x for x in a))
    sum2 = math.sqrt(sum(y*y for y in b))
    if sum1 == 0 or sum2 == 0:
        return 0.0
    return num / (sum1 * sum2)


# --------------------------- Simple Vectorizer ---------------------------
class SimpleVectorizer:
    """A tiny in-memory bag-of-words vectorizer with IDF-like weighting.
    No external libs required so it's deterministic and transparent.
    """
    def __init__(self):
        self.vocab = {}
        self.idf = {}
        self.document_count = 0

    def fit(self, docs: List[str]):
        self.document_count = len(docs)
        df = Counter()
        for d in docs:
            tokens = set(self._tokenize(d))
            for t in tokens:
                df[t] += 1
        self.vocab = {t:i for i,t in enumerate(sorted(df.keys()))}
        self.idf = {t: math.log((1 + self.document_count) / (1 + df[t])) + 1 for t in df}

    def transform(self, doc: str) -> List[float]:
        vec = [0.0] * len(self.vocab)
        tf = Counter(self._tokenize(doc))
        if not tf:
            return vec
        max_tf = max(tf.values())
        for token, count in tf.items():
            if token in self.vocab:
                idx = self.vocab[token]
                # tf normalization times idf
                tf_norm = count / max_tf
                vec[idx] = tf_norm * self.idf.get(token, 1.0)
        return vec

    def _tokenize(self, s: str) -> List[str]:
        s = s.lower()
        for ch in "()[]{}.,:;!\"'?/\\<>@#&*-+=_|~`":
            s = s.replace(ch, " ")
        return [tok for tok in s.split() if tok]


# --------------------------- Memory Layer ---------------------------
class MemoryAgent:
    """Structured memory with simple vector similarity and keyword search.
    Stores records with: id, timestamp, topic, source, agent, confidence, text, embedding
    """
    def __init__(self, vectorizer: SimpleVectorizer):
        self.vectorizer = vectorizer
        self.records: List[Dict[str, Any]] = []
        self.next_id = 1

    def add(self, topic: str, source: str, agent: str, confidence: float, text: str):
        embedding = self.vectorizer.transform(text)
        rec = {
            'id': self.next_id,
            'timestamp': now_ts(),
            'topic': topic,
            'source': source,
            'agent': agent,
            'confidence': confidence,
            'text': text,
            'embedding': embedding
        }
        self.next_id += 1
        self.records.append(rec)
        return rec

    def search_keyword(self, keyword: str, top_k: int = 5) -> List[Dict[str, Any]]:
        keyword = keyword.lower()
        results = []
        for r in self.records:
            if keyword in r['topic'].lower() or keyword in r['text'].lower():
                results.append(r)
        # simple sort by confidence, newest first
        results.sort(key=lambda x: (-x['confidence'], x['timestamp']), reverse=False)
        return results[:top_k]

    def search_vector(self, query_text: str, top_k: int = 5) -> List[Tuple[Dict[str, Any], float]]:
        qv = self.vectorizer.transform(query_text)
        sims = []
        for r in self.records:
            sim = cosine_sim(qv, r['embedding'])
            sims.append((r, sim))
        sims.sort(key=lambda x: x[1], reverse=True)
        return sims[:top_k]

    def list_records(self) -> List[Dict[str, Any]]:
        return list(self.records)


# --------------------------- Knowledge Base & Research Agent ---------------------------
class KnowledgeBase:
    """Mock knowledge base: a list of (id, title, text, source, date)
    In a real system this would be a DB or external web search.
    """
    def __init__(self):
        self.docs = []

    def load_mock_documents(self):
        # A small curated set of documents for ML topics
        docs = [
            {
                'id': 'doc1',
                'title': 'Neural Network Types: CNN, RNN, MLP',
                'text': 'Convolutional Neural Networks (CNNs) are suited for images. Recurrent Neural Networks (RNNs) and LSTMs are for sequences. Feedforward networks (MLP) are basic building blocks.',
                'source': 'mock-kb',
                'date': '2021-05-01'
            },
            {
                'id': 'doc2',
                'title': 'Transformer Architecture Overview',
                'text': 'Transformers use self-attention and positional encodings. They scale well with data and parallelize training. However, attention is O(n^2) in sequence length.',
                'source': 'mock-kb',
                'date': '2022-07-15'
            },
            {
                'id': 'doc3',
                'title': 'Adam Optimizer and Variants',
                'text': 'Adam is an adaptive optimizer combining momentum and RMSProp ideas. It often converges faster but may generalize differently than SGD with momentum.',
                'source': 'mock-kb',
                'date': '2020-03-10'
            },
            {
                'id': 'doc4',
                'title': 'Reinforcement Learning Survey 2020',
                'text': 'Recent papers in reinforcement learning explore sample efficiency, off-policy learning, and exploration strategies. Common challenges: stability, sample complexity, and reproducibility.',
                'source': 'mock-kb',
                'date': '2020-11-20'
            },
            {
                'id': 'doc5',
                'title': 'Transformer Efficiency Improvements',
                'text': 'Sparse attention and linearized attention approximations reduce the quadratic bottleneck. Methods like Reformer and Linformer propose modifications for long sequences.',
                'source': 'mock-kb',
                'date': '2023-02-02'
            }
        ]
        self.docs = docs

    def all_texts(self) -> List[str]:
        return [d['title'] + '\n' + d['text'] for d in self.docs]

    def search(self, query: str, top_k: int = 3, vectorizer: SimpleVectorizer = None) -> List[Dict[str, Any]]:
        # Two-stage: keyword match then vector similarity if vectorizer provided
        results = []
        q = query.lower()
        for d in self.docs:
            score = 0
            if q in d['title'].lower():
                score += 2
            if q in d['text'].lower():
                score += 1
            if score > 0:
                results.append((d, score))
        if vectorizer and not results:
            # fallback to vector similarity across docs
            qv = vectorizer.transform(query)
            sims = []
            for d in self.docs:
                dv = vectorizer.transform(d['title'] + ' ' + d['text'])
                sims.append((d, cosine_sim(qv, dv)))
            sims.sort(key=lambda x: x[1], reverse=True)
            return [x[0] for x in sims[:top_k]]
        results.sort(key=lambda x: x[1], reverse=True)
        return [x[0] for x in results[:top_k]]


class ResearchAgent:
    def __init__(self, kb: KnowledgeBase, vectorizer: SimpleVectorizer):
        self.kb = kb
        self.vectorizer = vectorizer

    def find(self, query: str, top_k: int = 3) -> Dict[str, Any]:
        # Simulate retrieval with provenance, include confidence
        docs = self.kb.search(query, top_k=top_k, vectorizer=self.vectorizer)
        confidence = 0.6 + 0.1 * min(len(docs), 3)  # rough heuristic
        payload = {
            'agent': 'research',
            'query': query,
            'results': docs,
            'confidence': round(confidence, 2),
            'timestamp': now_ts()
        }
        return payload


# --------------------------- Analysis Agent ---------------------------
class AnalysisAgent:
    def __init__(self):
        pass

    def compare_optimizers(self, items: List[Dict[str, Any]]) -> Dict[str, Any]:
        # Simple heuristic comparing 'speed' vs 'generalization' based on keywords
        summary = []
        for d in items:
            text = (d.get('title','') + ' ' + d.get('text','')).lower()
            score_speed = 0
            score_generalization = 0
            if 'fast' in text or 'converge' in text or 'converges' in text:
                score_speed += 1
            if 'general' in text or 'generalize' in text:
                score_generalization += 1
            if 'adaptive' in text:
                score_speed += 1
            summary.append({
                'id': d.get('id'),
                'title': d.get('title'),
                'speed_score': score_speed,
                'gen_score': score_generalization
            })
        # produce comparison notes
        notes = "Comparative analysis produced using simple heuristics."
        confidence = 0.7
        return {'agent': 'analysis', 'summary': summary, 'notes': notes, 'confidence': confidence, 'timestamp': now_ts()}

    def analyze_transformers_efficiency(self, docs: List[Dict[str, Any]]) -> Dict[str, Any]:
        # extract statements about complexity and trade-offs
        points = []
        for d in docs:
            text = d.get('text','')
            if 'O(n^2)' in text or 'quadratic' in text:
                points.append('Attention has O(n^2) complexity in sequence length; this is a known bottleneck.')
            if 'parallel' in text:
                points.append('Transformers parallelize well across sequence positions, speeding training on large hardware.')
            if 'sparse' in text or 'linear' in text:
                points.append('Sparse/linear attention variants reduce complexity, trading approximation accuracy for efficiency.')
        if not points:
            points.append('No explicit efficiency statements found; analysis based on available summaries.')
        confidence = 0.75
        return {'agent': 'analysis', 'points': points, 'confidence': confidence, 'timestamp': now_ts()}

    def analyze_papers_methodologies(self, docs: List[Dict[str, Any]]) -> Dict[str, Any]:
        # For mock papers, analyze methodologies by keywords
        methods = Counter()
        challenges = Counter()
        for d in docs:
            t = (d.get('title','') + ' ' + d.get('text','')).lower()
            if 'sample' in t:
                methods['sample_efficiency'] += 1
            if 'off-policy' in t:
                methods['off_policy'] += 1
            if 'exploration' in t:
                challenges['exploration'] += 1
            if 'stability' in t:
                challenges['stability'] += 1
            if 'reproducibil' in t:
                challenges['reproducibility'] += 1
        summary = {'methods': dict(methods), 'challenges': dict(challenges)}
        return {'agent': 'analysis', 'summary': summary, 'confidence': 0.7, 'timestamp': now_ts()}


# --------------------------- Coordinator ---------------------------
class Coordinator:
    def __init__(self, research: ResearchAgent, analysis: AnalysisAgent, memory: MemoryAgent, kb: KnowledgeBase):
        self.research = research
        self.analysis = analysis
        self.memory = memory
        self.kb = kb
        self.trace: List[str] = []

    def log(self, msg: str):
        ts = now_ts()
        line = f"[{ts}] {msg}"
        print(line)
        self.trace.append(line)

    def decide_plan(self, user_query: str) -> List[Tuple[str, Any]]:
        # Very simple planner based on keywords and length
        q = user_query.lower()
        plan = []
        if any(w in q for w in ['research', 'find', 'recent', 'papers', 'papers on']):
            plan.append(('research', {'query': user_query}))
            if any(w in q for w in ['analyze', 'compare', 'efficiency', 'methodology', 'trade-offs']):
                plan.append(('analysis', {}))
        elif any(w in q for w in ['compare', 'which is better', 'recommend']):
            plan.append(('research', {'query': user_query}))
            plan.append(('analysis', {}))
        else:
            # default: research then analysis if needed
            plan.append(('research', {'query': user_query}))
        self.log(f"Planner decided on steps: {', '.join([p[0] for p in plan])}")
        return plan

    def handle(self, user_query: str) -> Dict[str, Any]:
        self.log(f"Coordinator received query: {user_query}")
        # check memory first for related content
        mem_sims = self.memory.search_vector(user_query, top_k=3)
        mem_influence = 0
        if mem_sims and mem_sims[0][1] > 0.6:
            self.log(f"Found highly similar memory (sim={mem_sims[0][1]:.2f}), will use cached result.")
            mem_influence = mem_sims[0][1]
        plan = self.decide_plan(user_query)
        intermediate = {}
        for step, opts in plan:
            try:
                if step == 'research':
                    query = opts.get('query', user_query)
                    self.log(f"-> Invoking ResearchAgent with query: {query}")
                    res = self.research.find(query)
                    self.log(f"<- ResearchAgent returned {len(res['results'])} docs, confidence={res['confidence']}")
                    intermediate['research'] = res
                    # store in memory
                    combined_text = '\n'.join([d['title'] + '\n' + d['text'] for d in res['results']])
                    memrec = self.memory.add(topic=user_query, source='research', agent='research', confidence=res['confidence'], text=combined_text)
                    self.log(f"MemoryAgent stored research record id={memrec['id']}")

                elif step == 'analysis':
                    self.log(f"-> Invoking AnalysisAgent with research output")
                    research_docs = intermediate.get('research', {}).get('results', [])
                    # choose specific analysis based on user query keywords
                    q = user_query.lower()
                    if 'transformer' in q or 'transformers' in q:
                        res = self.analysis.analyze_transformers_efficiency(research_docs)
                    elif 'reinforcement' in q:
                        res = self.analysis.analyze_papers_methodologies(research_docs)
                    else:
                        res = self.analysis.compare_optimizers(research_docs)
                    intermediate['analysis'] = res
                    self.log(f"<- AnalysisAgent returned, confidence={res.get('confidence')}")
                    # store analysis
                    memrec = self.memory.add(topic=user_query, source='analysis', agent='analysis', confidence=res.get('confidence', 0.5), text=json.dumps(res))
                    self.log(f"MemoryAgent stored analysis record id={memrec['id']}")

            except Exception as e:
                self.log(f"ERROR during step {step}: {e}")
                # fallback: degrade gracefully
                intermediate[step] = {'error': str(e), 'agent': step}
        # synthesize final answer
        final = self.synthesize(user_query, intermediate, mem_influence)
        # store final summary
        self.memory.add(topic=user_query, source='coordinator', agent='coordinator', confidence=final.get('confidence', 0.6), text=final.get('answer',''))
        return final

    def synthesize(self, user_query: str, intermediate: Dict[str, Any], mem_influence: float) -> Dict[str, Any]:
        parts = []
        confs = []
        if 'research' in intermediate:
            docs = intermediate['research']['results']
            parts.append('Research found: ' + '; '.join([d['title'] for d in docs]))
            confs.append(intermediate['research']['confidence'])
        if 'analysis' in intermediate:
            a = intermediate['analysis']
            if 'points' in a:
                parts.append('Analysis points: ' + ' | '.join(a['points']))
            if 'summary' in a:
                parts.append('Analysis summary: ' + json.dumps(a['summary']))
            confs.append(a.get('confidence', 0.6))
        if not parts:
            parts.append('No direct findings.')
        # combine confidence as weighted average
        if confs:
            confidence = sum(confs)/len(confs)
            # nudge confidence up if memory influence
            confidence = min(0.99, confidence + 0.1 * mem_influence)
        else:
            confidence = 0.5
        answer = '\n'.join(parts)
        self.log(f"Coordinator synthesized answer with confidence={confidence:.2f}")
        return {'answer': answer, 'confidence': round(confidence,2), 'timestamp': now_ts(), 'trace': list(self.trace)}


# --------------------------- Runner / Test Scenarios ---------------------------

def ensure_outputs_dir():
    if os.path.exists('outputs'):
        shutil.rmtree('outputs')
    os.makedirs('outputs', exist_ok=True)


def write_output(name: str, text: str):
    path = os.path.join('outputs', name)
    with open(path, 'w', encoding='utf-8') as f:
        f.write(text)


def make_repo_supporting_files():
    # README
    readme = """# Simple Multi-Agent Chat System (Prototype)

This repository contains a single-file prototype `multi_agent_chat_system.py` implementing:
- Coordinator, ResearchAgent, AnalysisAgent, MemoryAgent
- A tiny in-memory vectorizer and knowledge base
- Test scenarios and outputs

Run the script: `python3 multi_agent_chat_system.py`

Files generated at runtime:
- outputs/*.txt (scenario traces)
- README.md, Dockerfile, docker-compose.yaml (created by script)

"""
    with open('README.md','w',encoding='utf-8') as f:
        f.write(readme)

    dockerfile = '''# Minimal Dockerfile for running the Python prototype
FROM python:3.11-slim
WORKDIR /app
COPY . /app
CMD ["python", "multi_agent_chat_system.py"]
'''
    with open('Dockerfile','w',encoding='utf-8') as f:
        f.write(dockerfile)

    docker_compose = '''version: '3.8'
services:
  app:
    build: .
    container_name: multi-agent-proto
    volumes:
      - ./:/app
    command: python multi_agent_chat_system.py
'''
    with open('docker-compose.yaml','w',encoding='utf-8') as f:
        f.write(docker_compose)


def run_tests(coordinator: Coordinator):
    scenarios = [
        ("simple_query.txt", "What are the main types of neural networks?"),
        ("complex_query.txt", "Research transformer architectures, analyze their computational efficiency, and summarize key trade-offs."),
        ("memory_test.txt", "What did we discuss about neural networks earlier?"),
        ("multi_step.txt", "Find recent papers on reinforcement learning, analyze their methodologies, and identify common challenges."),
        ("collaborative.txt", "Compare Adam and SGD and recommend which is better for our use case.")
    ]
    outputs = {}
    for filename, query in scenarios:
        coordinator.trace = []  # reset trace for clarity per scenario
        coordinator.log(f"=== Running scenario: {filename} ===")
        result = coordinator.handle(query)
        out = {
            'query': query,
            'result': result
        }
        text = f"Query: {query}\n\nAnswer (confidence={result.get('confidence')}):\n{result.get('answer')}\n\nTrace:\n" + '\n'.join(result.get('trace',[]))
        write_output(filename, text)
        print('\n--- Saved output to outputs/' + filename + '\n')
        outputs[filename] = text
    return outputs


# --------------------------- Main Execution ---------------------------
if __name__ == '__main__':
    ensure_outputs_dir()
    # Prepare knowledge base and vectorizer
    kb = KnowledgeBase()
    kb.load_mock_documents()
    vectorizer = SimpleVectorizer()
    vectorizer.fit(kb.all_texts())
    # Initialize agents
    memory_agent = MemoryAgent(vectorizer)
    research_agent = ResearchAgent(kb, vectorizer)
    analysis_agent = AnalysisAgent()
    coordinator = Coordinator(research_agent, analysis_agent, memory_agent, kb)

    # generate additional repo files
    make_repo_supporting_files()

    # Run test scenarios and save outputs
    outputs = run_tests(coordinator)

    # Summarize produced outputs
    summary_text = 'Generated outputs:\n' + '\n'.join(['- outputs/' + k for k in outputs.keys()]) + '\n\nMemory records stored:\n'
    for r in memory_agent.list_records():
        summary_text += f"- id={r['id']} topic={r['topic']} agent={r['agent']} confidence={r['confidence']} timestamp={r['timestamp']}\n"
    write_output('index.txt', summary_text)
    print('\nAll done. Files written into outputs/ and supporting files created (README.md, Dockerfile, docker-compose.yaml).')
